mastercopy$Q13C1 <- factor(mastercopy$Q13C1, levels = c(1,2,3))
mastercopy$Q13C2 <- factor(mastercopy$Q13C2, levels = c(1,2,3))
mastercopy$Q13C3 <- factor(mastercopy$Q13C3, levels = c(1,2,3))
mastercopy$Q13C4 <- factor(mastercopy$Q13C4, levels = c(1,2,3))
mastercopy$Q13C5 <- factor(mastercopy$Q13C5, levels = c(1,2,3))
mastercopy$Q13C6 <- factor(mastercopy$Q13C6, levels = c(1,2,3))
lapply(mastercopy, unique)[52:61]
lapply(mastercopy, unique)[62:65]
#So, We can see that 5th sub question in Q18 is not recorded. And all other data entries are have range from 1 to 5 so we need to remove the 6 level.
mastercopy$Q18I1 <- factor(mastercopy$Q18I1, levels = c(1,2,3,4,5))
mastercopy$Q18I2 <- factor(mastercopy$Q18I2, levels = c(1,2,3,4,5))
mastercopy$Q18I3 <- factor(mastercopy$Q18I3, levels = c(1,2,3,4,5))
mastercopy$Q18I4 <- factor(mastercopy$Q18I4, levels = c(1,2,3,4,5))
lapply(mastercopy,unique)[66:71]
names(mastercopy)[66:71] <- c("Q19C1","Q19C2","Q19C3","Q19C4","Q19c5","Q19c6")
dim((mastercopy))
# Missing Values
#Lets have a look on the missing values in our data.
mastercopy1<-mastercopy
mastercopy1<-read.csv("mastercopy1.csv",stringsAsFactors = FALSE)
library("naniar")
vis_miss(mastercopy1)
dim(mastercopy1)
# To delete the columns which are having more than 90% DAta
# To delete the rows which are having more than 50% data
mastercopy1<-mastercopy1[which(rowMeans(!is.na(mastercopy1)) > 0.5), ]
mastercopy1<-mastercopy1[,which(colMeans(!is.na(mastercopy1)) > 0.1)]
vis_miss(mastercopy1)
dim(mastercopy1)
#to fill zero in multiple choice questions
#We Start with Q5
names(mastercopy1[10:15])
mastercopy1[10:15][is.na(mastercopy1[10:15])] <- 0
#with Q12
lapply(mastercopy1,names)[33:37]
mastercopy1[33:37][is.na(mastercopy1[33:37])] <- 0
#with Q13
names(mastercopy1[38:42])
mastercopy1[38:42][is.na(mastercopy1[38:42])] <- 0
mastercopy1[38:42][mastercopy1[38:42] > 1] <- 1
#with Q15
names(mastercopy1)[44:48]
mastercopy1[44:48][is.na(mastercopy1[44:48])] <- 0
#with Q16
names(mastercopy1)[49:51]
mastercopy1[49:51][is.na(mastercopy1[49:51])] <- 0
#with Q19
names(mastercopy1)[61:66]
mastercopy1[61:66][is.na(mastercopy1[61:66])] <- 0
dim(mastercopy1)
vis_miss(mastercopy1)
#Summary of the data
summary(mastercopy1)
#Applying Mice to fill null values
install.packages("mice")
library(mice)
nomissdf= mice(mastercopy1)
df1=complete(nomissdf,1)
summary(df1)
vis_miss(df1)
mastercopy2 <- df1
#Finally import the Final data set
write.csv(mastercopy2,"final.csv", row.names = FALSE)
##Make Sure Q12,Q13 and Q19 are of making orders
#it is priority based. For Q19 we have 6 options
#19c1 to 19c6 and first row has values 2,3,4,5,1,6 in order
#or priorities for the first person.
names(mastercopy2[1:5])
#PCA
res.pca0 <- prcomp(mastercopy2[1:5],scale= TRUE)
summary(res.pca0)
plot(res.pca0)
screeplot(res.pca0,type="line",main="Scree Plot")
library("factoextra")
eig.val0 <- get_eigenvalue(res.pca0)
eig.val0
dimT0 <- c(1:5)
#Plot the cumulative percentage variance accounted for versus the index of the Components
plot(dimT0, eig.val0$cumulative.variance.percent, ylab = "Commulative Variance",xlab = "Principal Components")
#StreePlot
fviz_eig(res.pca0)
#Loading score
fviz_pca_var(res.pca0,axes = c(1,2),col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
# A. KNOWLEGDE
res.pca1 <- prcomp(mastercopy2[6:19],scale= TRUE)
summary(res.pca1)
library("factoextra")
eig.val1 <- get_eigenvalue(res.pca1)
eig.val1
dimT1 <- c(1:14)
#Plot the cumulative percentage variance accounted for versus the index of the Components
plot(dimT1, eig.val1$cumulative.variance.percent, ylab = "Commulative Variance",xlab = "Principal Components")
#StreePlot
fviz_eig(res.pca1)
#Loading score
fviz_pca_var(res.pca1,axes = c(1,2),col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
# B. ATTITUDE AND PERCEPTION
names(mastercopy2[20:26])
res.pca2 <- prcomp(mastercopy2[20:26],scale= TRUE)
summary(res.pca2)
library("factoextra")
eig.val2 <- get_eigenvalue(res.pca2)
eig.val2
dimT2 <- c(1:7)
dimT2
#Plot the cumulative percentage variance accounted for versus the index of the Components
plot(dimT2, eig.val2$cumulative.variance.percent, ylab = "Commulative Variance",xlab = "Principal Components")
#StreePlot
fviz_eig(res.pca2)
#Loading Score
fviz_pca_var(res.pca2,axes = c(1,2),col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
# c. SOCIAL NORMS AFFECTING PLASTIC CHANGING INTENTION AND BEHAVIORS
names(mastercopy2[27:31])
res.pca3 <- prcomp(mastercopy2[27:31],scale= TRUE)
summary(res.pca3)
library("factoextra")
eig.val3 <- get_eigenvalue(res.pca3)
eig.val3
dimT3 <- c(1:5)
dimT3
#Plot the cumulative percentage variance accounted for versus the index of the Components
plot(dimT3, eig.val3$cumulative.variance.percent, ylab = "Commulative Variance",xlab = "Principal Components")
#StreePlot
fviz_eig(res.pca3)
#Loading Score
fviz_pca_var(res.pca3,axes = c(1,2),col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
# D. PERCEIVED BEHAVIOR CONTROL OVER PLASTIC BEHAVIOR INTENTION AND BEHAVIORAL CHANGE
names(mastercopy2[32:48])
res.pca4 <- prcomp(mastercopy2[32:48],scale= TRUE)
summary(res.pca4)
library("factoextra")
eig.val4 <- get_eigenvalue(res.pca4)
eig.val4
dimT4 <- c(1:17)
dimT4
#Plot the cumulative percentage variance accounted for versus the index of the Components
plot(dimT4, eig.val4$cumulative.variance.percent, ylab = "Commulative Variance",xlab = "Principal Components")
#StreePlot
fviz_eig(res.pca4)
#Loading Score
fviz_pca_var(res.pca4,axes = c(1,2),col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
# E. Plastic Related Behaviour
names(mastercopy2[49:56])
res.pca5 <- prcomp(mastercopy2[49:56],scale= TRUE)
summary(res.pca5)
library("factoextra")
eig.val5 <- get_eigenvalue(res.pca5)
eig.val5
dimT5 <- c(1:8)
dimT5
#Plot the cumulative percentage variance accounted for versus the index of the Components
plot(dimT5, eig.val5$cumulative.variance.percent, ylab = "Commulative Variance",xlab = "Principal Components")
#StreePlot
fviz_eig(res.pca5)
#Loading Score
fviz_pca_var(res.pca5,axes = c(1,2),col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
# F Intention to change Behaviour
names(mastercopy2[57:60])
res.pca6 <- prcomp(mastercopy2[57:60],scale= TRUE)
summary(res.pca6)
library("factoextra")
eig.val6 <- get_eigenvalue(res.pca6)
eig.val6
dimT6 <- c(1:4)
dimT6
#Plot the cumulative percentage variance accounted for versus the index of the Components
plot(dimT6, eig.val6$cumulative.variance.percent, ylab = "Commulative Variance",xlab = "Principal Components")
#StreePlot
fviz_eig(res.pca6)
#Loading Score
fviz_pca_var(res.pca6,axes = c(1,2),col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
# G Communication related plastics
names(mastercopy2[61:66])
res.pca7 <- prcomp(mastercopy2[61:66],scale= TRUE)
summary(res.pca7)
library("factoextra")
eig.val7 <- get_eigenvalue(res.pca6)
eig.val7
dimT7 <- c(1:4)
dimT7
#Plot the cumulative percentage variance accounted for versus the index of the Components
plot(dimT7, eig.val7$cumulative.variance.percent, ylab = "Commulative Variance",xlab = "Principal Components")
#StreePlot
fviz_eig(res.pca7)
#Loading Score
fviz_pca_var(res.pca7,axes = c(1,2),col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
#DA
#DA
#DA
#DA
#DA
#DA
#DA
View(mastercopy2)
#DA
#try1
da1 <- lda(mastercopy2$q1k~mastercopy2[1:5],mastercopy2[1:6])
library("factoextra")
da1 <- lda(mastercopy2$q1k~mastercopy2[1:5],mastercopy2[1:6])
#DA
#try1
library(MASS)
da1 <- lda(mastercopy2$q1k~mastercopy2[1:5],mastercopy2[1:6])
str(mastercopy2[1:6])
da1 <- lda(mastercopy2$q1k~mastercopy2[1:5],mastercopy2[1:6])
da1 <- lda(mastercopy2$q1k~mastercopy2[1:5],mastercopy2)
da1 <- lda(mastercopy2$q1k~mastercopy2[1:5],mastercopy2)
str(mastercopy2[1:6])
da1 <- lda(mastercopy2$q1k~mastercopy2[1:3],mastercopy2)
da1 <- lda(mastercopy2$q1k~mastercopy2[1],mastercopy2)
da1 <- lda(mastercopy2$q1k~mastercopy2[1:5],mastercopy2)
(mastercopy2[1:5])
da1 <- lda(mastercopy2$q1k~mastercopy2[1:5],mastercopy2)
da1 <- lda(mastercopy2$q1k~as.data.frame(mastercopy2[1:5]),mastercopy2)
da1 <- lda(mastercopy2$q1k~as.data.frame(mastercopy2[1:5]),as.data.frame(mastercopy2[1:6]))
da1 <- lda(mq1k~as.data.frame(mastercopy2[1:5]),as.data.frame(mastercopy2[1:6]))
da1 <- lda(q1k~as.data.frame(mastercopy2[1:5]),as.data.frame(mastercopy2[1:6]))
da1 <- lda(q1k~names(mastercopy2[1:5],as.data.frame(mastercopy2[1:6]))
summary(da1)
da1 <- lda(q1k~names(mastercopy2[1:5],mastercopy2[1:6])
summary(da1)
da1 <- lda(q1k~names(mastercopy2[1:5],mastercopy2[1:6])
da1 <- lda(q1k~names(mastercopy2[1:5],mastercopy2[1:6]))
summary(da1)
da1 <- lda(q1k~names(mastercopy2[1:5],mastercopy2[1:6]))
da1 <- lda(q1k~Gender+Age+Income+Education+Occupation,mastercopy2[1:6]))
da1 <- lda(q1k~Gender+Age+Income+Education+Occupation,data=mastercopy2[1:6]))
da1 <- lda(q1k~Gender+Age+Income+Education+Occupation,data=mastercopy2)
summary(da1)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,q1k)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- qda(q1k~Gender+Age+Income+Education+Occupation,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q1k~Age+Income+Education+Occupation,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q1k~Age+Income+Education+Occupation,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q1k~Gender+Age+Income+Education,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q1k~Gender+Age+Education,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q1k~Gender+Age+Education+Occupation,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q1k~Gender+Education+Occupation,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q1k~Gender+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q2k~Gender+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q2k~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q4k~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q4k~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q1k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q2k~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$q2k)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(q7k~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1 <- lda(Q7K~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1
da1 <- lda(Q7K~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$Q7K)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(Q8K~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$Q8K)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(Q10C1~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$Q10C1)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- lda(Q14C~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1 <- lda(Q14C~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$Q14C)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
da1 <- qda(Q14C~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$Q14C)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$Q14C)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
#DA
#try1
library(MASS)
da1 <- qda(Q14C~Gender+Age+Education+Occupation+Income,data=mastercopy2)
da1
#LDA preduction
lda.testing <- predict(da1)
#confusion matrix
accuracy <- table(lda.testing$class,mastercopy2$Q14C)
accuracy
sum(accuracy[row(accuracy) == col(accuracy)]) / sum(accuracy)
View(res.pca3)
real.estate <- read.csv("C://Users//vasu2//Downloads//real.csv")
lm1<- lm(Price ~ .-ID, data = real.estate)
summary(lm1)
plot(lm1)# to check linearlity and normality
mahal <- mahalanobis(real.estate[,-1], colMeans(real.estate[,-1]), cov(real.estate[,-1]))
cuttoff #cuttoff score
ncol(real.estate[,-1])
badmah=as.numeric(mahal<cuttoff)
table(badmah)
#leverage
k=10#no of variables in lm
leverage = hatvalues(lm1)
cutlev = (2*k+2)/nrow(real.estate)
cutlev
badlev= as.numeric(leverage>cutlev)
table(badlev)
#Cook dist
cooks = cooks.distance(lm1)
cutcook= 4/(nrow(real.estate[,-1])-k-1)
cutcook
badcook = as.numeric(cooks> cutcook)
table(badcook)
#sum
totalout= badmah+badlev+badcook
table(totalout)
#getting rid
noout= subset(real.estate, totalout<2)
#newmodel
newmodel <- lm(Price~.-ID, data = noout)
summary(newmodel)
#to check assumptions
plot(newmodel)
fdsfs
#newmodel
newmodel <- lm(Price~.-ID, data = noout)
summary(newmodel)
#getting rid
noout= subset(real.estate, totalout<2)
#sum
totalout= badmah+badlev+badcook
#For outliers
cuttoff= qchisq(1-0.001,ncol(real.estate[,-1]))
mahal <- mahalanobis(real.estate[,-1], colMeans(real.estate[,-1]), cov(real.estate[,-1]))
cuttoff #cuttoff score
ncol(real.estate[,-1])
badmah=as.numeric(mahal<cuttoff)
table(badmah)
#leverage
k=10#no of variables in lm
leverage = hatvalues(lm1)
cutlev = (2*k+2)/nrow(real.estate)
cutlev
badlev= as.numeric(leverage>cutlev)
table(badlev)
#Cook dist
cooks = cooks.distance(lm1)
cutcook= 4/(nrow(real.estate[,-1])-k-1)
cutcook
badcook = as.numeric(cooks> cutcook)
table(badcook)
#sum
totalout= badmah+badlev+badcook
table(totalout)
#getting rid
noout= subset(real.estate, totalout<2)
#newmodel
newmodel <- lm(Price~.-ID, data = noout)
summary(newmodel)
#to check assumptions
plot(newmodel)
# Fit the full model
full.model <- lm(Price ~.-ID, data = noout)
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "both",
trace = FALSE)
summary(step.model)
resid(step.model)
# Fit the full model
full.model <- lm(Price ~.-ID, data = noout)
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "both",
trace = FALSE)
summary(step.model)
resid(step.model)
# Fit the full model
full.model <- lm(noout$Price ~.-noout$ID, data = noout)
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "both",
trace = FALSE)
summary(step.model)
resid(step.model)
write.csv(noout,"noout.csv", row.names = FALSE)
df <- read.csv("noout.csv")
# Fit the full model
full.model <- lm(Price ~.-ID, data = df)
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "both",
trace = FALSE)
summary(step.model)
resid(step.model)
summary(step.model)
AIC(step.model)
View(real.estate)
#Which variable is strnngest
install.packages("QuantPsyc")
library(QuantPsyc)
lm.beta(step.model)
summary(step.model)
